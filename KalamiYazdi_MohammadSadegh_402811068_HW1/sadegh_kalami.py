# -*- coding: utf-8 -*-
"""Sadegh-Kalami.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vfC4k7-eCP10aQ8nu7rl9dIPGtvCsR3G
"""

import numpy as np
import matplotlib.pyplot as plt
import keras

"""# Load CIFAR-10 Dataset

"""

from keras.datasets import cifar10
from sklearn.model_selection import train_test_split
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/6)


print("Training:", x_train.shape,y_train.shape)
print("Validation:", x_val.shape,y_val.shape)
print("Test:", x_test.shape,y_test.shape)

"""# Show sample CIFAR-10 Images"""

# Define CIFAR-10 class names
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

# Initialize a figure
fig, axarr = plt.subplots(5, 10, figsize=(20, 10))

# Iterate through each class
for class_index in range(10):
    # Find the indices of images with the current class
    class_indices = np.where(y_train == class_index)[0]
    # Randomly select 5 samples
    random_indices = np.random.choice(class_indices, size=5, replace=False)
    # Plot each sample in a column
    for i, idx in enumerate(random_indices):
        axarr[i, class_index].imshow(x_train[idx])
        if i==0: axarr[i, class_index].set_title(class_names[class_index])
        axarr[i, class_index].axis('off')

# Adjust spacing between subplots
plt.subplots_adjust(wspace=0.1, hspace=0.1)

"""# Load Different Layers and Create the Model (Original Code)"""

from keras.models import Sequential
from keras.layers import Flatten, Rescaling, Dense
from keras.layers import Conv2D, MaxPooling2D

# Create a Sequential model
model = Sequential()

# Add a layer to normlize the data
model.add(Rescaling(scale=1.0/255, offset=0.0))


# Add Convolutional layers
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# Flatten the output of Convolutional layers
model.add(Flatten())

# Add Dense layers
model.add(Dense(256, activation='relu'))
model.add(Dense(10, activation='softmax'))  # Output layer with 10 units for 10 classes

"""## Compiling the Model"""

from keras.optimizers import Adam
from keras.losses import sparse_categorical_crossentropy # equivalent to "categorical_crossentropy" without requiring one-hot encoded labels
# from keras.metrics import CategroicalAccuracy

model.compile(optimizer=Adam(learning_rate=0.001),loss=sparse_categorical_crossentropy,metrics=["accuracy"])

"""## Train the Original Model"""

history = model.fit(x_train, y_train, batch_size=128, epochs=50,
                    validation_data=(x_val,y_val))



"""## Plot Learning Curve"""

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""## Evaluating the model (Original)"""

# Evaluate the model on test data
loss, accuracy = model.evaluate(x_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

"""## further evaluation"""

from sklearn.metrics import classification_report,confusion_matrix
y_pred=model.predict(x_test).argmax(axis=1)
print(classification_report(y_true=y_test,y_pred=y_pred))
print(confusion_matrix(y_true=y_test,y_pred=y_pred))

"""# Sadegh-Kalami Codes

## Install Tuner
"""

!pip install keras-tuner

"""## First Model (Only Dropout added)"""

import keras_tuner as kt
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Rescaling
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from sklearn.model_selection import train_test_split

# # Load CIFAR-10 data
# (x_train, y_train), (x_test, y_test) = cifar10.load_data()
# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/6, random_state=20)

def build_model(hp):
    model = Sequential([
        Rescaling(scale=1.0/255, input_shape=(32, 32, 3)),
        Conv2D(32, kernel_size=(3, 3), activation='relu'),
        Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.9, step=0.1)),
        Conv2D(64, kernel_size=(3, 3), activation='relu'),
        Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.9, step=0.1)),
        MaxPooling2D(pool_size=(2, 2)),
        Conv2D(128, kernel_size=(3, 3), activation='relu'),
        Dropout(hp.Float('dropout_3', min_value=0.1, max_value=0.9, step=0.1)),
        MaxPooling2D(pool_size=(2, 2)),
        Flatten(),
        Dense(256, activation='relu'),
        Dropout(hp.Float('dropout_4', min_value=0.1, max_value=0.9, step=0.1)),
        Dense(10, activation='softmax')
    ])

    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss=SparseCategoricalCrossentropy(),
                  metrics=['accuracy'])
    return model

# Create a tuner. You can choose from RandomSearch, Hyperband, BayesianOptimization, and Sklearn
tuner = kt.Hyperband(build_model,
                     objective='val_accuracy',
                     max_epochs=50,
                     factor=4,
                     directory='my_dir',
                     project_name='intro_to_kt')

# Create a callback to stop training early after reaching a certain value for the validation loss
stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

tuner.search(x_train, y_train, epochs=50, validation_data=(x_val, y_val), callbacks=[stop_early])

# Get the optimal hyperparameters
best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal dropout rates are:
    - Dropout 1: {best_hps.get('dropout_1')}
    - Dropout 2: {best_hps.get('dropout_2')}
    - Dropout 3: {best_hps.get('dropout_3')}
    - Dropout 4: {best_hps.get('dropout_4')}
""")

model = tuner.hypermodel.build(best_hps)
history = model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val))

model.save('Sadegh-Kalami-Dropout-added.keras')

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plotting training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Evaluate the model on test data
loss, accuracy = model.evaluate(x_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

from sklearn.metrics import classification_report, confusion_matrix
y_pred = model.predict(x_test).argmax(axis=1)
print(classification_report(y_true=y_test, y_pred=y_pred))
print(confusion_matrix(y_true=y_test, y_pred=y_pred))

"""## Second Model (StopEarly and LR-Schedular)"""

import keras_tuner as kt
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Rescaling
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from sklearn.model_selection import train_test_split

def build_model(hp):
    model = Sequential([
        Rescaling(scale=1.0/255, input_shape=(32, 32, 3)),
        Conv2D(32, kernel_size=(3, 3), activation='relu'),
        Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.9, step=0.1)),
        Conv2D(64, kernel_size=(3, 3), activation='relu'),
        Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.9, step=0.1)),
        MaxPooling2D(pool_size=(2, 2)),
        Conv2D(128, kernel_size=(3, 3), activation='relu'),
        Dropout(hp.Float('dropout_3', min_value=0.1, max_value=0.9, step=0.1)),
        MaxPooling2D(pool_size=(2, 2)),
        Flatten(),
        Dense(256, activation='relu'),
        Dropout(hp.Float('dropout_4', min_value=0.1, max_value=0.9, step=0.1)),
        Dense(10, activation='softmax')
    ])

    # Hyperparameter for learning rate
    lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')

    model.compile(optimizer=Adam(learning_rate=lr),
                  loss=SparseCategoricalCrossentropy(),
                  metrics=['accuracy'])
    return model

tuner = kt.Hyperband(build_model,
                     objective='val_accuracy',
                     max_epochs=50,
                     factor=4,
                     directory='my_dir',
                     project_name='intro_to_kt')

# EarlyStopping Callback
stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

def scheduler(epoch, lr):
    if epoch < 10:
        return float(lr)
    else:
        return float(lr * tf.math.exp(-0.1).numpy())

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)

tuner.search(x_train, y_train, epochs=50, validation_data=(x_val, y_val), callbacks=[stop_early, lr_scheduler])

best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal parameters are:
    - Dropout 1: {best_hps.get('dropout_1')}
    - Dropout 2: {best_hps.get('dropout_2')}
    - Dropout 3: {best_hps.get('dropout_3')}
    - Dropout 4: {best_hps.get('dropout_4')}
    - Learning Rate: {best_hps.get('learning_rate')}
""")

model = tuner.hypermodel.build(best_hps)
history = model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val),callbacks=[stop_early, lr_scheduler])

model.save('Sadegh-Kalami-EarlyStoping-LR.keras')

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

loss, accuracy = model.evaluate(x_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

from sklearn.metrics import classification_report, confusion_matrix
y_pred = model.predict(x_test).argmax(axis=1)
print(classification_report(y_true=y_test, y_pred=y_pred))
print(confusion_matrix(y_true=y_test, y_pred=y_pred))

"""## Third Model (Batchnorm + EarlyStop + LR-Schedular)"""

import keras_tuner as kt
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Rescaling
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import BatchNormalization

def build_model(hp):
    model = Sequential([
        Rescaling(scale=1.0/255, input_shape=(32, 32, 3)),
        Conv2D(32, kernel_size=(3, 3), use_bias=False),  # Remove bias because BatchNormalization has its own bias
        BatchNormalization(),
        tf.keras.layers.Activation('relu'),  # Apply activation after BatchNormalization
        Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.9, step=0.1)),

        Conv2D(64, kernel_size=(3, 3), use_bias=False),
        BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.9, step=0.1)),
        MaxPooling2D(pool_size=(2, 2)),

        Conv2D(128, kernel_size=(3, 3), use_bias=False),
        BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        Dropout(hp.Float('dropout_3', min_value=0.1, max_value=0.9, step=0.1)),
        MaxPooling2D(pool_size=(2, 2)),

        Flatten(),
        Dense(256, activation='relu'),
        Dropout(hp.Float('dropout_4', min_value=0.1, max_value=0.9, step=0.1)),
        Dense(10, activation='softmax')
    ])

    lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')

    model.compile(optimizer=Adam(learning_rate=lr),
                  loss=SparseCategoricalCrossentropy(),
                  metrics=['accuracy'])
    return model




tuner = kt.Hyperband(build_model,
                     objective='val_accuracy',
                     max_epochs=50,
                     factor=4,
                     directory='my_dir',
                     project_name='intro_to_kt')

# EarlyStopping Callback
stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

def scheduler(epoch, lr):
    if epoch < 10:
        return float(lr)
    else:
        return float(lr * tf.math.exp(-0.1).numpy())


lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)

tuner.search(x_train, y_train, epochs=50, validation_data=(x_val, y_val), callbacks=[stop_early, lr_scheduler])

best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal parameters are:
    - Dropout 1: {best_hps.get('dropout_1')}
    - Dropout 2: {best_hps.get('dropout_2')}
    - Dropout 3: {best_hps.get('dropout_3')}
    - Dropout 4: {best_hps.get('dropout_4')}
    - Learning Rate: {best_hps.get('learning_rate')}
""")

model = tuner.hypermodel.build(best_hps)
history = model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val),callbacks=[stop_early, lr_scheduler])

model.save('Sadegh-Kalami-batchnormed.keras')

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

loss, accuracy = model.evaluate(x_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

from sklearn.metrics import classification_report, confusion_matrix
y_pred = model.predict(x_test).argmax(axis=1)
print(classification_report(y_true=y_test, y_pred=y_pred))
print(confusion_matrix(y_true=y_test, y_pred=y_pred))

"""## Final Model (all above + L2-Regularization)"""

from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Rescaling, BatchNormalization, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import keras_tuner as kt
import tensorflow as tf
from tensorflow.keras.layers import BatchNormalization




################### side option ##############
# # Data Augmentation
# datagen = ImageDataGenerator(
#     rotation_range=15,
#     width_shift_range=0.1,
#     height_shift_range=0.1,
#     horizontal_flip=True,
#     zoom_range=0.2
# )

def build_model(hp):
    l2_reg = hp.Float('l2_reg', min_value=1e-5, max_value=1e-2, sampling='log')

    model = Sequential([
        Rescaling(scale=1.0/255, input_shape=(32, 32, 3)),
        Conv2D(32, (3, 3), use_bias=False, kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        Activation('relu'),
        Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)),

        Conv2D(64, (3, 3), use_bias=False, kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        Activation('relu'),
        Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)),
        MaxPooling2D((2, 2)),

        Conv2D(128, (3, 3), use_bias=False, kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        Activation('relu'),
        Dropout(hp.Float('dropout_3', min_value=0.1, max_value=0.5, step=0.1)),
        MaxPooling2D((2, 2)),

        Flatten(),
        Dense(256, activation='relu', kernel_regularizer=l2(l2_reg)),
        Dropout(hp.Float('dropout_4', min_value=0.1, max_value=0.5, step=0.1)),
        Dense(10, activation='softmax')
    ])

    lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')

    model.compile(optimizer=Adam(learning_rate=lr),
                  loss=SparseCategoricalCrossentropy(),
                  metrics=['accuracy'])
    return model










tuner = kt.Hyperband(build_model,
                     objective='val_accuracy',
                     max_epochs=70,
                     factor=4,
                     directory='my_dir',
                     project_name='intro_to_kt')

stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

def scheduler(epoch, lr):
    if epoch < 10:
        return float(lr)
    else:
        return float(lr * tf.math.exp(-0.1).numpy())


lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)

tuner.search(x_train, y_train, epochs=70, validation_data=(x_val, y_val), callbacks=[stop_early, lr_scheduler])

best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal parameters are:
    - Dropout 1: {best_hps.get('dropout_1')}
    - Dropout 2: {best_hps.get('dropout_2')}
    - Dropout 3: {best_hps.get('dropout_3')}
    - Dropout 4: {best_hps.get('dropout_4')}
    - Learning Rate: {best_hps.get('learning_rate')}
    - Learning Rate: {best_hps.get('l2_reg')}
""")

model = tuner.hypermodel.build(best_hps)
history = model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val),callbacks=[stop_early, lr_scheduler])

model.save('Sadegh-Kalami_l2r-added.keras')

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

loss, accuracy = model.evaluate(x_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

from sklearn.metrics import classification_report, confusion_matrix
y_pred = model.predict(x_test).argmax(axis=1)
print(classification_report(y_true=y_test, y_pred=y_pred))
print(confusion_matrix(y_true=y_test, y_pred=y_pred))

